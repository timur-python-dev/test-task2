Ключевая сложность в системе Mymeet.ai — это обработка длительных, ресурсоемких задач и работа с неравномерной нагрузкой. Поэтому всю архитектуру я бы построил на принципах асинхронности и разделения ответственности.

В основе системы будет лежать микросервисная архитектура, а связующим звеном между сервисами — очередь сообщений, например, RabbitMQ. Это позволит нам сделать компоненты независимыми и отказоустойчивыми. Если сервис обработки упадет, задача не потеряется, а просто дождется его перезапуска в очереди.

Давайте проследим путь одной встречи через систему.

Все начинается с API Gateway — единой точки входа для веб-интерфейса, телеграм-бота и виджетов. Он отвечает за аутентификацию и маршрутизацию. Запрос на запись попадает в сервис-Оркестратор, который работает с календарями и ручными ссылками. Его главная задача — не выполнять запись самому, а создать событие "начать запись встречи X" и отправить его в очередь.

Эту задачу из очереди подхватывает один из Ботов-рекордеров. Это будет масштабируемый пул headless-браузеров (на Playwright или Puppeteer), работающих в Kubernetes. Такая схема позволяет нам автоматически увеличивать их количество в часы пик (например, в 10 утра) и сокращать в остальное время для экономии ресурсов. После записи бот сохраняет файл в объектное хранилище типа S3 и отправляет в очередь новое событие: "встреча X записана".

Это событие запускает конвейер обработки. Сначала сервис-обертка над Whisper API делает транскрибацию. Затем другой сервис-обертка над GPT-4 получает этот текст и генерирует отчет по нужному шаблону. Каждый шаг — это отдельный маленький сервис, который делает одно простое дело. Когда отчет готов, финальное событие улетает в Сервис уведомлений, который и отправляет пользователю письмо.

По технологиям я бы остановился на следующем:
*   Go для высоконагруженных сетевых сервисов (Gateway, Оркестратор).
*   Python на FastAPI для всего, что связано с обработкой данных и AI, где важна экосистема библиотек.
*   PostgreSQL для метаданных, S3/MinIO для файлов, Redis для кэша.
*   Все это в Docker-контейнерах под управлением Kubernetes.

Теперь о том, где система может столкнуться с проблемами.

1.  Пиковая нагрузка на запись. Как я уже упомянул, "проблема 10 утра" — это главный вызов для инфраструктуры. Решение здесь — это грамотно настроенный **Horizontal Pod Autoscaler** в Kubernetes, который будет следить за длиной очереди в RabbitMQ и динамически менять количество ботов-рекордеров.

2.  Заторы в конвейере обработки. Представим, что кто-то загрузил 3-часовую запись. Ее обработка может надолго занять ресурсы и заставить ждать другие, более срочные 5-минутные встречи. Решение — приоритетные очереди. Мы можем создать отдельную очередь с высоким приоритетом для встреч, записанных в реальном времени, и с низким — для загруженных вручную файлов, с отдельными пулами обработчиков для каждой.

3.  Зависимость от внешних AI-сервисов. Они дороги и имеют лимиты. На старте это оправдано, но долгосрочная стратегия — это накопление собственного датасета и дообучение open-source моделей. Это позволит снизить издержки и получить более качественные результаты на специфической для бизнеса лексике.

Такая архитектура получается гибкой. Добавить интеграцию с Jira — значит написать еще один маленький сервис, который будет слушать событие "отчет готов". Внедрить real-time транскрипт — это следующий шаг, который потребует WebSocket'ов, но базовый каркас системы к этому уже готов.
